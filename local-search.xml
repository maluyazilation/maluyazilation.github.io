<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>yolov5官方文章</title>
    <link href="/2021/10/04/yolov5-example/"/>
    <url>/2021/10/04/yolov5-example/</url>
    
    <content type="html"><![CDATA[<style>  .markdown-body {    font-family: "Microsoft YaHei", serif;    font-size: 24px;  }</style><a href="https://apps.apple.com/app/id1452689527" target="_blank"><img src="https://user-images.githubusercontent.com/26833433/82944393-f7644d80-9f4f-11ea-8b87-1a5b04f555f1.jpg" width="1000"></a>&nbsp<p><img src="https://github.com/ultralytics/yolov5/workflows/CI%20CPU%20testing/badge.svg" alt="CI CPU testing"></p><p>This repository represents Ultralytics open-source research into future object detection methods, and incorporates our lessons learned and best practices evolved over training thousands of models on custom client datasets with our previous YOLO repository <a href="https://github.com/ultralytics/yolov3">https://github.com/ultralytics/yolov3</a>. <strong>All code and models are under active development, and are subject to modification or deletion without notice.</strong> Use at your own risk.</p><p><img src="https://user-images.githubusercontent.com/26833433/90187293-6773ba00-dd6e-11ea-8f90-cd94afc0427f.png" width="1000">** GPU Speed measures end-to-end time per image averaged over 5000 COCO val2017 images using a V100 GPU with batch size 32, and includes image preprocessing, PyTorch FP16 inference, postprocessing and NMS. EfficientDet data from <a href="https://github.com/google/automl">google/automl</a> at batch size 8.</p><ul><li><strong>August 13, 2020</strong>: <a href="https://github.com/ultralytics/yolov5/releases/tag/v3.0">v3.0 release</a>: nn.Hardswish() activations, data autodownload, native AMP.</li><li><strong>July 23, 2020</strong>: <a href="https://github.com/ultralytics/yolov5/releases/tag/v2.0">v2.0 release</a>: improved model definition, training and mAP.</li><li><strong>June 22, 2020</strong>: <a href="https://arxiv.org/abs/1803.01534">PANet</a> updates: new heads, reduced parameters, improved speed and mAP <a href="https://github.com/ultralytics/yolov5/commit/364fcfd7dba53f46edd4f04c037a039c0a287972">364fcfd</a>.</li><li><strong>June 19, 2020</strong>: <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module.half">FP16</a> as new default for smaller checkpoints and faster inference <a href="https://github.com/ultralytics/yolov5/commit/d4c6674c98e19df4c40e33a777610a18d1961145">d4c6674</a>.</li><li><strong>June 9, 2020</strong>: <a href="https://github.com/WongKinYiu/CrossStagePartialNetworks">CSP</a> updates: improved speed, size, and accuracy (credit to @WongKinYiu for CSP).</li><li><strong>May 27, 2020</strong>: Public release. YOLOv5 models are SOTA among all known YOLO implementations.</li><li><strong>April 1, 2020</strong>: Start development of future compound-scaled <a href="https://github.com/ultralytics/yolov3">YOLOv3</a>/<a href="https://github.com/AlexeyAB/darknet">YOLOv4</a>-based PyTorch models.</li></ul><h2 id="Pretrained-Checkpoints"><a href="#Pretrained-Checkpoints" class="headerlink" title="Pretrained Checkpoints"></a>Pretrained Checkpoints</h2><table><thead><tr><th>Model</th><th>AP<sup>val</sup></th><th>AP<sup>test</sup></th><th>AP<sub>50</sub></th><th>Speed<sub>GPU</sub></th><th>FPS<sub>GPU</sub></th><th></th><th>params</th><th align="center">FLOPS</th></tr></thead><tbody><tr><td><a href="https://github.com/ultralytics/yolov5/releases/tag/v3.0">YOLOv5s</a></td><td>37.0</td><td>37.0</td><td>56.2</td><td><strong>2.4ms</strong></td><td><strong>416</strong></td><td></td><td>7.5M</td><td align="center">13.2B</td></tr><tr><td><a href="https://github.com/ultralytics/yolov5/releases/tag/v3.0">YOLOv5m</a></td><td>44.3</td><td>44.3</td><td>63.2</td><td>3.4ms</td><td>294</td><td></td><td>21.8M</td><td align="center">39.4B</td></tr><tr><td><a href="https://github.com/ultralytics/yolov5/releases/tag/v3.0">YOLOv5l</a></td><td>47.7</td><td>47.7</td><td>66.5</td><td>4.4ms</td><td>227</td><td></td><td>47.8M</td><td align="center">88.1B</td></tr><tr><td><a href="https://github.com/ultralytics/yolov5/releases/tag/v3.0">YOLOv5x</a></td><td><strong>49.2</strong></td><td><strong>49.2</strong></td><td><strong>67.7</strong></td><td>6.9ms</td><td>145</td><td></td><td>89.0M</td><td align="center">166.4B</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td></tr><tr><td><a href="https://github.com/ultralytics/yolov5/releases/tag/v3.0">YOLOv5x</a> + TTA</td><td><strong>50.8</strong></td><td><strong>50.8</strong></td><td><strong>68.9</strong></td><td>25.5ms</td><td>39</td><td></td><td>89.0M</td><td align="center">354.3B</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td align="center"></td></tr><tr><td><a href="https://github.com/ultralytics/yolov5/releases/tag/v3.0">YOLOv3-SPP</a></td><td>45.6</td><td>45.5</td><td>65.2</td><td>4.5ms</td><td>222</td><td></td><td>63.0M</td><td align="center">118.0B</td></tr></tbody></table><p>** AP<sup>test</sup> denotes COCO <a href="http://cocodataset.org/#upload">test-dev2017</a> server results, all other AP results in the table denote val2017 accuracy.<br>** All AP numbers are for single-model single-scale without ensemble or test-time augmentation. <strong>Reproduce</strong> by <code>python test.py --data coco.yaml --img 640 --conf 0.001</code><br>** Speed<sub>GPU</sub> measures end-to-end time per image averaged over 5000 COCO val2017 images using a GCP <a href="https://cloud.google.com/compute/docs/machine-types#n1_standard_machine_types">n1-standard-16</a> instance with one V100 GPU, and includes image preprocessing, PyTorch FP16 image inference at –batch-size 32 –img-size 640, postprocessing and NMS. Average NMS time included in this chart is 1-2ms/img.  <strong>Reproduce</strong> by <code>python test.py --data coco.yaml --img 640 --conf 0.1</code><br>** All checkpoints are trained to 300 epochs with default settings and hyperparameters (no autoaugmentation).<br>** Test Time Augmentation (<a href="https://github.com/ultralytics/yolov5/issues/303">TTA</a>) runs at 3 image sizes. <strong>Reproduce</strong> by <code>python test.py --data coco.yaml --img 832 --augment</code> </p><h2 id="Requirements"><a href="#Requirements" class="headerlink" title="Requirements"></a>Requirements</h2><p>Python 3.8 or later with all <a href="https://github.com/ultralytics/yolov5/blob/master/requirements.txt">requirements.txt</a> dependencies installed, including <code>torch&gt;=1.6</code>. To install run:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ pip install -r requirements.txt<br></code></pre></td></tr></table></figure><h2 id="Tutorials"><a href="#Tutorials" class="headerlink" title="Tutorials"></a>Tutorials</h2><ul><li><a href="https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data">Train Custom Data</a></li><li><a href="https://github.com/ultralytics/yolov5/issues/475">Multi-GPU Training</a></li><li><a href="https://github.com/ultralytics/yolov5/issues/36">PyTorch Hub</a></li><li><a href="https://github.com/ultralytics/yolov5/issues/251">ONNX and TorchScript Export</a></li><li><a href="https://github.com/ultralytics/yolov5/issues/303">Test-Time Augmentation (TTA)</a></li><li><a href="https://github.com/ultralytics/yolov5/issues/318">Model Ensembling</a></li><li><a href="https://github.com/ultralytics/yolov5/issues/304">Model Pruning/Sparsity</a></li><li><a href="https://github.com/ultralytics/yolov5/issues/607">Hyperparameter Evolution</a></li><li><a href="https://github.com/wang-xinyu/tensorrtx">TensorRT Deployment</a></li></ul><h2 id="Environments"><a href="#Environments" class="headerlink" title="Environments"></a>Environments</h2><p>YOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including <a href="https://developer.nvidia.com/cuda">CUDA</a>/<a href="https://developer.nvidia.com/cudnn">CUDNN</a>, <a href="https://www.python.org/">Python</a> and <a href="https://pytorch.org/">PyTorch</a> preinstalled):</p><ul><li><strong>Google Colab Notebook</strong> with free GPU: <a href="https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></li><li><strong>Kaggle Notebook</strong> with free GPU: <a href="https://www.kaggle.com/ultralytics/yolov5">https://www.kaggle.com/ultralytics/yolov5</a></li><li><strong>Google Cloud</strong> Deep Learning VM. See <a href="https://github.com/ultralytics/yolov5/wiki/GCP-Quickstart">GCP Quickstart Guide</a> </li><li><strong>Docker Image</strong> <a href="https://hub.docker.com/r/ultralytics/yolov5">https://hub.docker.com/r/ultralytics/yolov5</a>. See <a href="https://github.com/ultralytics/yolov5/wiki/Docker-Quickstart">Docker Quickstart Guide</a> <img src="https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker" alt="Docker Pulls"></li></ul><h2 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h2><p>Inference can be run on most common media formats. Model <a href="https://drive.google.com/open?id=1Drs_Aiu7xx6S-ix95f9kNsA6ueKRpN2J">checkpoints</a> are downloaded automatically if available. Results are saved to <code>./inference/output</code>.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ python detect.py --<span class="hljs-built_in">source</span> 0  <span class="hljs-comment"># webcam</span><br>                            file.jpg  <span class="hljs-comment"># image </span><br>                            file.mp4  <span class="hljs-comment"># video</span><br>                            path/  <span class="hljs-comment"># directory</span><br>                            path/*.jpg  <span class="hljs-comment"># glob</span><br>                            rtsp://170.93.143.139/rtplive/470011e600ef003a004ee33696235daa  <span class="hljs-comment"># rtsp stream</span><br>                            http://112.50.243.8/PLTV/88888888/224/3221225900/1.m3u8  <span class="hljs-comment"># http stream</span><br></code></pre></td></tr></table></figure><p>To run inference on examples in the <code>./inference/images</code> folder:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ python detect.py --<span class="hljs-built_in">source</span> ./inference/images/ --weights yolov5s.pt --conf 0.4<br><br>Namespace(agnostic_nms=False, augment=False, classes=None, conf_thres=0.4, device=<span class="hljs-string">&#x27;&#x27;</span>, fourcc=<span class="hljs-string">&#x27;mp4v&#x27;</span>, half=False, img_size=640, iou_thres=0.5, output=<span class="hljs-string">&#x27;inference/output&#x27;</span>, save_txt=False, <span class="hljs-built_in">source</span>=<span class="hljs-string">&#x27;./inference/images/&#x27;</span>, view_img=False, weights=<span class="hljs-string">&#x27;yolov5s.pt&#x27;</span>)<br>Using CUDA device0 _CudaDeviceProperties(name=<span class="hljs-string">&#x27;Tesla P100-PCIE-16GB&#x27;</span>, total_memory=16280MB)<br><br>Downloading https://drive.google.com/uc?<span class="hljs-built_in">export</span>=download&amp;id=1R5T6rIyy3lLwgFXNms8whc-387H0tMQO as yolov5s.pt... Done (2.6s)<br><br>image 1/2 inference/images/bus.jpg: 640x512 3 persons, 1 buss, Done. (0.009s)<br>image 2/2 inference/images/zidane.jpg: 384x640 2 persons, 2 ties, Done. (0.009s)<br>Results saved to /content/yolov5/inference/output<br></code></pre></td></tr></table></figure><img src="https://user-images.githubusercontent.com/26833433/83082816-59e54880-a039-11ea-8abe-ab90cc1ec4b0.jpeg" width="500">  <h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>Download <a href="https://github.com/ultralytics/yolov5/blob/master/data/scripts/get_coco.sh">COCO</a> and run command below. Training times for YOLOv5s/m/l/x are 2/4/6/8 days on a single V100 (multi-GPU times faster). Use the largest <code>--batch-size</code> your GPU allows (batch sizes shown for 16 GB devices).</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ python train.py --data coco.yaml --cfg yolov5s.yaml --weights <span class="hljs-string">&#x27;&#x27;</span> --batch-size 64<br>                                         yolov5m                                40<br>                                         yolov5l                                24<br>                                         yolov5x                                16<br></code></pre></td></tr></table></figure><img src="https://user-images.githubusercontent.com/26833433/90222759-949d8800-ddc1-11ea-9fa1-1c97eed2b963.png" width="900"><h2 id="Citation"><a href="#Citation" class="headerlink" title="Citation"></a>Citation</h2><p><a href="https://zenodo.org/badge/latestdoi/264818686"><img src="https://zenodo.org/badge/264818686.svg" alt="DOI"></a></p><h2 id="About-Us"><a href="#About-Us" class="headerlink" title="About Us"></a>About Us</h2><p>Ultralytics is a U.S.-based particle physics and AI startup with over 6 years of expertise supporting government, academic and business clients. We offer a wide range of vision AI services, spanning from simple expert advice up to delivery of fully customized, end-to-end production solutions, including:</p><ul><li><strong>Cloud-based AI</strong> systems operating on <strong>hundreds of HD video streams in realtime.</strong></li><li><strong>Edge AI</strong> integrated into custom iOS and Android apps for realtime <strong>30 FPS video inference.</strong></li><li><strong>Custom data training</strong>, hyperparameter evolution, and model exportation to any destination.</li></ul><p>For business inquiries and professional support requests please visit us at <a href="https://www.ultralytics.com/">https://www.ultralytics.com</a>. </p><h2 id="Contact"><a href="#Contact" class="headerlink" title="Contact"></a>Contact</h2><p><strong>Issues should be raised directly in the repository.</strong> For business inquiries or professional support requests please visit <a href="https://www.ultralytics.com/">https://www.ultralytics.com</a> or email Glenn Jocher at <a href="mailto:&#x67;&#108;&#101;&#x6e;&#110;&#x2e;&#x6a;&#111;&#x63;&#104;&#101;&#114;&#x40;&#x75;&#108;&#x74;&#114;&#97;&#x6c;&#x79;&#x74;&#x69;&#x63;&#x73;&#46;&#99;&#111;&#x6d;">&#x67;&#108;&#101;&#x6e;&#110;&#x2e;&#x6a;&#111;&#x63;&#104;&#101;&#114;&#x40;&#x75;&#108;&#x74;&#114;&#97;&#x6c;&#x79;&#x74;&#x69;&#x63;&#x73;&#46;&#99;&#111;&#x6d;</a>. </p>]]></content>
    
    
    
    <tags>
      
      <tag>code</tag>
      
      <tag>computer vision</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2021/10/03/hello-world/"/>
    <url>/2021/10/03/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
